## Correlation is not causation

```{r,echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidyr)
library(broom)
library(dslabs)
ds_theme_set()
```

Correlation is not causation is perhaps the most important lesson one learns in a statistics class. We have described tools useful for quantifying associations between variables. However, we must be careful not to overinterpret these associations. 

There are many reasons that a variable $X$ can be correlated with a variable $Y$ without either being a cause for the other. Here we examine three common ways that can lead to misinterpreting data.

### Spurious correlation

The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption. 

```{r, echo=FALSE}
the_title <- paste("Correlation =", 
                round(with(divorce_margarine, 
                           cor(margarine_consumption_per_capita, divorce_rate_maine)),2))
data(divorce_margarine)
divorce_margarine %>% 
  ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) + 
  geom_point(cex=3) + 
  geom_smooth(method = "lm") + 
  ggtitle(the_title) +
  xlab("Margarine Consumption per Capita (lbs)") + 
  ylab("Divorce rate in Maine (per 1000)")
```

Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both of these questions is no. This is just an example of what we call a _spurious correlation_.

You can see many more absurd examples on [this website](http://tylervigen.com/spurious-correlations) completely dedicated to _spurious correlations_. 

The cases presented in the _spurious correlations_ site are all examples of what is generally called _data dredging_, _data fishing_, or _data snooping_. It's basically a form of what in the US they call _cherry picking_. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.

A simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble: 
```{r, cache=TRUE}
N <- 25
G <- 1000000
sim_data <- tibble(group = rep(1:G, each = N), X = rnorm(N*G), Y = rnorm(N*G))
sim_data
```

The first column denotes group and we simulated $G$ groups each with `r N` observations. For each group we generate `r N` observations, which are stored in the second and third columns. These are just random independent normally distributed data. So we know, because we constructed the simulation, that $X$ and $Y$ are not correlated.

Next, we compute the correlation between `X` and `Y` for each group and look at the max:

```{r}
res <- sim_data %>% group_by(group) %>% summarize(r = cor(X, Y)) %>% arrange(desc(r))
res
```

We see a correlation of `r signif(max(res$r), 3)` and if you just plot the data from that group it shows a convincing plot that $X$ and $Y$ are in fact correlated:

```{r}
sim_data %>% filter(group == res$group[which.max(res$r)]) %>%
  ggplot(aes(X, Y)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

Remember that the correlation summary is a random variable. Here is the distribution generated by the simulation:

```{r}
res %>% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = "black")
```

It is just a mathematical fact that if we observe `r cat(prettyNum(G,big.mark=",",scientific=FALSE))` random correlations that are expected to be 0 but have a standard error of `r signif(sd(res$r),3)`, the largest one will be close 1. 

Note that if we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:

```{r}
sim_data %>% 
  filter(group == res$group[which.max(res$r)]) %>%
  do(tidy(lm(Y ~ X, data = .)))
```

This particular form of data dredging is referred to as _p-hacking_. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative (null) results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to adjust for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather to statistical ignorance or wishful thinking. In advanced statistics courses you learn methods to adjust for these multiple comparisons.


### Outliers

Suppose we take measurements from two independent outcomes, $X$ and $Y$, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:

```{r}
set.seed(1)
x <- rnorm(100,100,1)
y <- rnorm(100,84,1)
x[-23] <- scale(x[-23])
y[-23] <- scale(y[-23])
```

The data look like this:
```{r}
tibble(x,y) %>% ggplot(aes(x,y)) + geom_point(alpha = 0.5)
```

Not surprisingly, the correlation is very high:

```{r}
cor(x,y)
```

But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, what it should be:

```{r}
cor(x[-23], y[-23])
```

There is an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called _Spearman correlation_. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:

```{r}
tibble(x,y) %>% 
  ggplot(aes(rank(x),rank(y))) + 
  geom_point(alpha = 0.5)
```

The outlier is no longer associated with a very large value and the correlation comes way down:

```{r}
cor(rank(x), rank(y))
```

Spearman correlation can also be calculated like this:

```{r}
cor(x, y, method = "spearman")
```

There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: [Robust Statistics](https://www.wiley.com/en-us/Robust+Statistics%2C+2nd+Edition-p-9780470129906): Edition 2 Peter J. Huber Elvezio M. Ronchetti. 


### Reversing cause and effect

Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.  

A form of this claim actually made it into an op-ed in the New York Times titled [Parental Involvement Is Overrated](https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated/?_php=true&_type=blogs&_php=true&_type=blogs&_r=1&). Consider this quote from the article:

>> When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades... Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.

A very likely possibility is that the children needing regular parental help, receive this help because they don't perform well in school.

We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model

$$X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N$$

to the father and son height data, with $X_i$ the father height and $y_i$ the son height, we do get a statistically significant result:

```{r}
library(HistData)
data("GaltonFamilies")
GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight) %>% 
  do(tidy(lm(father ~ son, data = .)))
```

The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it's the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the _interpretation_.


### Confounders

Confounders are perhaps the most common reason that leads to associations being misinterpreted.  

If $X$ and $Y$ are correlated, we call $Z$ a _confounder_ if changes in $Z$ cause changes in both $X$ and $Y$. In some cases, we can use linear models to account for confounders. However, this is not always the case.

Incorrect interpretation due to confounders is ubiquitous in the lay press. It is sometimes hard to detect. Here we present two examples, both related to gender discrimination.

### Example: UC Berkeley admissions 

Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O'Connell. Science (1975). Here is the data:

```{r}
data(admissions)
admissions
```

The percent of men and women that were accepted was:

```{r}
admissions %>% group_by(gender) %>% 
               summarize(percentage = 
               round(sum(admitted*applicants)/sum(applicants),1))
```

A statistical test clearly rejects the hypothesis that gender and admission are independent:

```{r}
admissions %>% group_by(gender) %>% 
               summarize(total_admitted = round(sum(admitted/100*applicants)), 
                         not_admitted = sum(applicants) - sum(total_admitted)) %>%
               select(-gender) %>% 
               summarize(tidy(chisq.test(.)))
```

But closer inspection shows a paradoxical result. Here are the percent admissions by major:

```{r}
admissions %>% select(major, gender, admitted) %>%
               pivot_wider(names_from = gender, values_from = admitted) %>%
               mutate(women_minus_men = women - men)
```

Four out of the six majors favor women. More importantly all the differences are much smaller than the 14.2 difference that we see when examining the totals.

The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear.  What's going on? This actually can happen if an uncounted confounder is driving most of the variability.

So let's define three variables: $X$ is 1 for men and 0 for women, $Y$ is 1 for those admitted and 0 otherwise, and $Z$ quantifies how selective the major is. A gender bias claim would be based on the fact that $\mbox{Pr}(Y=1 | X = x)$ is higher for $x=1$ than $x=0$. But $Z$ is an important confounder to consider. Clearly $Z$ is associated with  $Y$, as the more selective a major, the lower $\mbox{Pr}(Y=1 | Z = z)$. But is major selectivity $Z$ associated with gender $X$?

One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:

```{r}
admissions %>% group_by(major) %>% 
               summarize(major_selectivity = sum(admitted*applicants)/sum(applicants),
               percent_women_applicants = sum(applicants*(gender=="women")/sum(applicants))*100) %>%
               ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
               geom_text()
```

There seems to be an association. The plot suggests that women were much more likely to apply to the two "hard" majors: gender and major's selectivity are confounded. Compare, for example,  major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women while less than 10% of the applicants of major B were women. 


#### Confounding explained graphically

The following plot shows the number of applicants that were admitted and those that were not:

```{r confounding, echo=FALSE}
admissions %>%
  mutate(yes = round(admitted/100*applicants), no = applicants - yes) %>%
  select(-applicants, -admitted) %>%
  gather(admission, number_of_students, -c("major", "gender")) %>%
  ggplot(aes(gender, number_of_students, fill = admission)) +
  geom_bar(stat = "identity", position = "stack") +
  facet_wrap(. ~ major) +
  ylab("Number of students") + 
  xlab("")
```


It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors. 



#### Average after stratifying

In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away. 

```{r admission-by-major, fig.cap="Admission percentage by major for each gender."}
admissions %>% ggplot(aes(major, admitted, col = gender, size = applicants)) +
               geom_point()
```

Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.

If we average the difference by major we find that the percent is actually 3.5% higher for women.

```{r}
admissions %>%  group_by(gender) %>% summarize(average = mean(admitted))
```


### Simpson's Paradox 

The case we have just covered is an example of Simpson's paradox. It is called a paradox because we see the sign of the correlation  flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables $X$, $Y$, and $Z$ and that we observe realizations of these. Here is a plot of simulated observations for $X$ and $Y$ along with the sample correlation:

```{r simpsons-paradox, echo=FALSE}
N <- 100
Sigma <- matrix(c(1,0.75,0.75, 1), 2, 2)*1.5
means <- list(c(x = 11, y = 3), 
              c(x = 9, y = 5), 
              c(x = 7, y = 7), 
              c(x = 5, y = 9), 
              c(x = 3, y = 11))
dat <- lapply(means, function(mu){
  res <- MASS::mvrnorm(N, mu, Sigma)
  colnames(res) <- c("x", "y")
  res
})
dat <- do.call(rbind, dat) |> 
  as_tibble() |>
  mutate(z = as.character(rep(seq_along(means), each = N)))
dat |> ggplot(aes(x, y)) + geom_point(alpha = 0.5) +
  ggtitle(paste("Correlation = ", round(cor(dat$x, dat$y), 2)))
```

You can see that $X$ and $Y$ are negatively correlated. However, once we stratify by $Z$ (shown in different colors below) another pattern emerges:


```{r simpsons-paradox-explained, echo=FALSE}
means <- do.call(rbind, means) |> 
  as_tibble() |>
  mutate(z = as.character(seq_along(means)))
  
corrs <- dat |> group_by(z) |> summarize(cor = cor(x, y)) |> pull(cor)
dat |> ggplot(aes(x, y, color = z)) + 
  geom_point(show.legend = FALSE, alpha = 0.5) +
  ggtitle(paste("Correlations =",  paste(signif(corrs,2), collapse=" "))) +
  annotate("text", x = means$x, y = means$y, label = paste("z =", means$z), cex = 5)  
```

It is really $Z$ that is negatively correlated with $X$. If we stratify by $Z$, the $X$ and $Y$ are actually positively correlated as seen in the plot above.



#### Gender contributes to personal research funding success in The Netherlands

Here we examine a case similar to the UC Berkeley admissions example, but much more subtle.

A [2014 PNAS paper](http://www.pnas.org/content/112/40/12349.abstract) analyzed success rates from funding agencies in the Netherlands and concluded that their:

> results reveal gender bias favoring male applicants over female applicants in the prioritization of their “quality of researcher” (but not "quality of proposal") evaluations and success rates, as well as in the language used in instructional and evaluation materials.

The main evidence for this conclusion comes down to a comparison of the percentages. Table S1 in the paper includes the information we need:

```{r,echo=FALSE}
data("research_funding_rates")
research_funding_rates
```

We can construct the two-by-two table used for the conclusion above:

```{r, warning=FALSE, message=FALSE}
two_by_two <- research_funding_rates %>% 
              select(-discipline) %>% 
              summarize_all(funs(sum)) %>%
              summarize(yes_men = awards_men, 
                        no_men = applications_men - awards_men, 
                        yes_women = awards_women, 
                        no_women = applications_women - awards_women) %>%
              pivot_longer("yes_men":"no_women", names_to = "key", values_to = "value") %>%
              separate(key, c("awarded", "gender")) %>%
              pivot_wider(names_from = gender, values_from = value)
two_by_two
```

Compute the difference in percentage:

```{r}
two_by_two %>% mutate(men = round(men/sum(men)*100, 1), women = round(women/sum(women)*100, 1)) %>% 
               filter(awarded == "yes")
```

note that it's lower for women, and find that it is almost statistically significant at the 0.05 level:

```{r}
two_by_two %>% select(-awarded) %>% 
               chisq.test() %>% 
               tidy
```

So there appears to be some evidence of an association. But can we infer causation here? Is gender bias causing this observed difference?

[A response](http://www.pnas.org/content/112/51/E7036.extract) was published a few months later titled _No evidence that gender contributes to personal research funding success in The Netherlands: A reaction to Van der Lee and Ellemers_  which concluded:

> However, the overall gender effect borders on statistical significance, despite the large sample. Moreover, their conclusion could be a prime example of Simpson’s paradox; if a higher percentage of women apply for grants in more competitive scientific disciplines (i.e., with low application success rates for both men and women), then an analysis across all disciplines could incorrectly show "evidence" of gender inequality.


In the UC Berkeley admissions example, the overall differences were explained by differences across disciplines. We use the same approach on the research funding data and look at comparisons by discipline:

```{r}
dat <- research_funding_rates %>% 
       rename(success_total = success_rates_total,
              success_men = success_rates_men,
              success_women = success_rates_women) %>%
        pivot_longer(-discipline, names_to = "key", values_to = "value") %>%
        separate(key, c("type", "gender")) %>%
        pivot_wider(names_from = "type", values_from = "value") %>%
        filter(gender != "total") %>%
        mutate(discipline = reorder(discipline, applications, sum)) 

dat %>% ggplot(aes(discipline, success, size = applications, color = gender)) + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
        geom_point()
```

Here we see that some fields favor men and other women. We see that the two fields with the largest difference favoring men, are also the fields with the most applications. However, are any of these differences statistically significant? Keep in mind that even when there is no bias, we will see differences due to random variability in the review process as well as random variability across candidates. If we perform a Fisher test in each discipline, we see that most differences result in p-values larger than 0.05.

```{r}
do_fisher_test <- function(m, x, n, y){
  tab <- tibble(men = c(x, m-x), women = c(y, n-y))
  tidy(fisher.test(tab)) %>% 
  rename(odds = estimate) %>%
  mutate(difference = y/n - x/m)
}
res <- research_funding_rates %>% 
  group_by(discipline) %>%
  do(do_fisher_test(.$applications_men, .$awards_men, 
                    .$applications_women, .$awards_women)) %>%
  ungroup() %>%
  select(discipline, difference, p.value) %>%
  arrange(difference)
res 
```

We see that for Earth/Life Sciences, there is a difference of 10% favoring men and this has a p-value of 0.04. But is this a spurious correlation? We performed 9 tests. Reporting only the one case with a p-value less than 0.05 would be cherry picking. 

The overall average of the difference is only -0.3%, which is much smaller than the standard error:

```{r}
res %>% summarize(overall_avg = mean(difference), 
                  se = sd(difference)/sqrt(n()))
```

Furthermore, note that the differences appear to follow a normal distribution:

```{r}
res %>% ggplot(aes(sample = scale(difference))) + 
        stat_qq() + 
        geom_abline()
```

which suggests the possibility that the observed differences are just due to chance. 

### The First Steps program
Finally, let's add known confounders to our birthweight model to see how the association between participation in the First Steps program and birthweight changes. Participants were targeted if they were young, on welfare, a smoker, and/or a drinker. We also know from previous birthweight studies in the US that race, education, mother's weight at the beginning of the pregnancy and the amount of weight gained during pregnancy are all associated with birthweight. Let's add each of these variables to our model.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
data <- read.table("KingCounty2001_data.txt", header = TRUE)
```


```{r}
m <- lm(bwt ~ firstep + gestation + age + welfare + smoker + drinker + wpre + wgain + education, data = data)
summary(m)
```


From the model we can see that participation in the First Steps program is still not significant, but that the effect size has decreased from an average birthweight decrease of 42 grams to an average birthweight decrease of 20.1 grams. 

Now let's look at the effect of the First Steps program stratified by race category:

```{r}
by_race <- data %>% group_by(race) %>%
         summarize(tidy(lm(bwt ~ gestation + firstep + age + welfare + smoker + drinker + wpre + wgain + education, data = .), conf.int = TRUE))
by_race[c(3, 13, 23, 33, 42),]
```

The coefficient for the First Step program is positive for 2 race categories (Asian and Hispanic), but negative for the others (Black, Other and White). From these results we should include race in our model. Here is the output of the model with race included:

```{r}
m <- lm(bwt ~ firstep + gestation + age + race + welfare + smoker + drinker + wpre + wgain + education, data = data)
summary(m)
```

From the analyses we have conducted it seems as though the First Steps program has not increased birthweight - the coefficient for the program was never significant. However, more information about participant recruitment (inclusion criteria and week of pregnancy the program was started) as well as more data and other analyses, for example logistic regression to predict low or normal birthweight, may shed more light on this.










